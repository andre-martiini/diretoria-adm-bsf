import { VertexAI } from '@google-cloud/vertexai';
import admin from 'firebase-admin';
import dotenv from 'dotenv';
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

dotenv.config({ path: path.join(__dirname, '..', '.env.local') });

// Configura√ß√£o Vertex AI
const project = process.env.PROJECT_ID || 'diretoria-adm-bsf';
const location = 'global';
const apiEndpoint = 'aiplatform.googleapis.com';
const serviceAccountPath = process.env.FIREBASE_SERVICE_ACCOUNT || path.join(__dirname, '..', 'serviceAccountKey.json');
console.log(`[SEARCH-AUTH] Verificando chave em: ${serviceAccountPath}`);
if (fs.existsSync(serviceAccountPath)) {
    console.log(`[SEARCH-AUTH] ‚úÖ Arquivo de credenciais encontrado.`);
} else {
    console.warn(`[SEARCH-AUTH] ‚ö†Ô∏è Arquivo de credenciais N√ÉO encontrado.`);
}

const vertexAI = new VertexAI({
    project,
    location,
    apiEndpoint,
    googleAuthOptions: fs.existsSync(serviceAccountPath) ? { keyFilename: serviceAccountPath } : undefined
});


/**
 * Realiza uma busca sem√¢ntica na base de conhecimento.
 * @param {string} query A pergunta do usu√°rio.
 * @param {string} processId (Opcional) Filtrar por um processo espec√≠fico.
 */
export async function searchKnowledgeBase(query, processId = null) {
    const db = admin.firestore();
    const generativeModel = vertexAI.preview.getGenerativeModel({ model: 'text-embedding-004' });

    console.log(`[AI-SEARCH] üîç Buscando por: "${query}"...`);

    // 1. Converter a pergunta em vetor
    const result = await generativeModel.embedContent({ content: { role: 'user', parts: [{ text: query }] } });
    const queryVector = result.embeddings[0].values;

    // 2. Buscar candidatos
    // Nota: Em produ√ß√£o com milh√µes de registros, usar√≠amos Vector Index.
    // Para < 10k chunks, busca em mem√≥ria/query √© vi√°vel e MUITO mais simples de manter.

    let chunksRef = db.collectionGroup('chunks');
    if (processId) {
        // Se for busca em um processo espec√≠fico, filtramos antes
        // Nota: CollectionGroup queries requerem √≠ndice se usar filtro.
        // Vamos buscar na cole√ß√£o espec√≠fica do processo para evitar necessidade de √≠ndice global complexo agora.
        chunksRef = db.collection('contratacoes').doc(processId).collection('arquivos');
        // A arquitetura atual salva chunks dentro de arquivos. 
        // Para buscar em TODO o processo, ter√≠amos que iterar arquivos.
        // Vamos manter a busca GLOBAL por enquanto, que √© mais poderosa.
    }

    // Buscando os √∫ltimos 500 chunks para comparar (limite de seguran√ßa)
    // Numa v2, implementaremos Firestore Vector Search nativo
    const snapshot = await db.collectionGroup('chunks').limit(500).get();

    const candidates = [];

    snapshot.forEach(doc => {
        const data = doc.data();
        // data.embedding √© um objeto VectorValue do Firestore. Precisamos do array.
        const vector = data.embedding.toArray();

        const similarity = cosineSimilarity(queryVector, vector);

        if (similarity > 0.6) { // Filtro de relev√¢ncia m√≠nima
            candidates.push({
                id: doc.id,
                text: data.text,
                similarity: similarity,
                metadata: {
                    page: data.pageNumber,
                    fileId: data.fileId,
                    processoId: data.processoId
                }
            });
        }
    });

    // Ordenar por similaridade
    candidates.sort((a, b) => b.similarity - a.similarity);

    return candidates.slice(0, 5); // Retorna Top 5
}

// Fun√ß√£o matem√°tica simples para similaridade de cosseno
function cosineSimilarity(vecA, vecB) {
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    for (let i = 0; i < vecA.length; i++) {
        dotProduct += vecA[i] * vecB[i];
        normA += vecA[i] * vecA[i];
        normB += vecB[i] * vecB[i];
    }
    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
}
