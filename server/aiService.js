import { VertexAI } from '@google-cloud/vertexai';
import { createRequire } from 'module';
import admin from 'firebase-admin';
import dotenv from 'dotenv';
import path from 'path';
import { fileURLToPath } from 'url';

import fs from 'fs';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const require = createRequire(import.meta.url);
const pdf = require('pdf-parse');
const axios = require('axios');
const { GoogleAuth } = require('google-auth-library');

dotenv.config({ path: path.join(__dirname, '..', '.env.local') });

// Define o caminho para o arquivo de credenciais
const serviceAccountPath = process.env.FIREBASE_SERVICE_ACCOUNT || path.join(__dirname, '..', 'serviceAccountKey.json');
console.log(`[AI-AUTH] Verificando chave em: ${serviceAccountPath}`);
if (fs.existsSync(serviceAccountPath)) {
    console.log(`[AI-AUTH] âœ… Arquivo de credenciais encontrado.`);
} else {
    console.warn(`[AI-AUTH] âš ï¸ Arquivo de credenciais NÃƒO encontrado em ${serviceAccountPath}`);
}

// ConfiguraÃ§Ã£o Vertex AI - Passando credenciais diretamente para evitar erros de ambiente
const project = process.env.PROJECT_ID || 'diretoria-adm-bsf';
const location = 'global';

const vertexAI = new VertexAI({
    project,
    location,
    apiEndpoint: 'aiplatform.googleapis.com',
    googleAuthOptions: fs.existsSync(serviceAccountPath) ? { keyFilename: serviceAccountPath } : undefined
});

// Client regional para Embeddings (us-central1) pois modelos de vetorizaÃ§Ã£o nÃ£o suportam endpoint global
const vertexAIEmbed = new VertexAI({
    project,
    location: 'us-central1',
    googleAuthOptions: fs.existsSync(serviceAccountPath) ? { keyFilename: serviceAccountPath } : undefined
});

/**
 * IMPLEMENTAÃ‡ÃƒO LOCAL DO RECURSIVE CHARACTER TEXT SPLITTER
 * Removemos a dependÃªncia do 'langchain' para evitar erros de importaÃ§Ã£o (ERR_PACKAGE_PATH_NOT_EXPORTED)
 * Esta classe replica a lÃ³gica de dividir o texto respeitando parÃ¡grafos e pontuaÃ§Ã£o.
 */
class RecursiveCharacterTextSplitter {
    constructor({ chunkSize = 1000, chunkOverlap = 200, separators = ["\n\n", "\n", ".", "!", "?", " ", ""] }) {
        this.chunkSize = chunkSize;
        this.chunkOverlap = chunkOverlap;
        this.separators = separators;
    }

    async createDocuments([text]) {
        const chunks = this.splitText(text, this.separators);
        return chunks.map(chunk => ({ pageContent: chunk }));
    }

    splitText(text, separators) {
        const finalChunks = [];
        let separator = separators[0];
        let nextSeparators = separators.slice(1);

        // Se nÃ£o temos mais separadores, cortamos na forÃ§a bruta (caractere)
        if (!separator) {
            return this.splitByCharacter(text);
        }

        let segments = text.split(separator);
        let currentChunk = "";

        for (let segment of segments) {
            // Se o segmento sozinho jÃ¡ Ã© grande, precisamos dividi-lo mais (sub-separadores)
            if (segment.length > this.chunkSize && nextSeparators.length > 0) {
                const subChunks = this.splitText(segment, nextSeparators);
                for (let sub of subChunks) {
                    this.accumulate(sub, separator, finalChunks, currentChunk);
                }
            } else {
                // Caso contrÃ¡rio, tentamos acumular
                if (currentChunk.length + segment.length + separator.length > this.chunkSize) {
                    if (currentChunk.trim().length > 0) finalChunks.push(currentChunk.trim());
                    currentChunk = segment; // ComeÃ§a novo chunk com overlap (simplificado aqui)
                } else {
                    currentChunk += (currentChunk.length > 0 ? separator : "") + segment;
                }
            }
        }

        if (currentChunk.trim().length > 0) finalChunks.push(currentChunk.trim());

        return finalChunks;
    }

    splitByCharacter(text) {
        const chunks = [];
        for (let i = 0; i < text.length; i += (this.chunkSize - this.chunkOverlap)) {
            chunks.push(text.slice(i, i + this.chunkSize));
        }
        return chunks;
    }

    accumulate(segment, separator, finalChunks, currentChunk) {
        // LÃ³gica simplificada de acumulaÃ§Ã£o
        if (currentChunk.length + segment.length + separator.length > this.chunkSize) {
            if (currentChunk.trim().length > 0) finalChunks.push(currentChunk.trim());
            return segment;
        } else {
            return currentChunk + (currentChunk.length > 0 ? separator : "") + segment;
        }
    }
}

/**
 * Orchestrator: Pega um arquivo do Storage, extrai, chunka e vetoriza.
 */
export async function processFileForDataLake(storagePath, processId, fileId) {
    console.log(`[AI-LAKE] ðŸ¤– Iniciando processamento inteligente: ${storagePath}`);
    const db = admin.firestore();
    const bucket = admin.storage().bucket('diretoria-adm-bsf.firebasestorage.app');

    try {
        // 1. Atualiza status para PROCESSING
        await db.collection('contratacoes').doc(processId).collection('arquivos').doc(fileId).update({
            status: 'PROCESSING',
            processingStartedAt: admin.firestore.FieldValue.serverTimestamp()
        });

        // 2. Download do buffer
        const [buffer] = await bucket.file(storagePath).download();

        // 3. ExtraÃ§Ã£o de Texto (TÃ¡tica HÃ­brida)
        let extractedText = "";
        try {
            const pdfData = await pdf(buffer);
            extractedText = pdfData.text;
        } catch (e) {
            console.warn(`[AI-LAKE] pdf-parse falhou, tentando Gemini OCR...`);
        }

        // Se o texto for muito curto ou falhar, usamos o Gemini 1.5 Flash para OCR direto no PDF
        if (extractedText.trim().length < 100) {
            console.log(`[AI-LAKE] Documento parece ser imagem ou digitalizaÃ§Ã£o. Usando Gemini Vision/OCR...`);
            extractedText = await extractTextWithGemini(buffer, storagePath);
        }

        if (!extractedText || extractedText.trim().length === 0) {
            throw new Error("NÃ£o foi possÃ­vel extrair nenhum texto do documento.");
        }

        // 4. Chunking Profissional (ImplementaÃ§Ã£o Local Robusta)
        const splitter = new RecursiveCharacterTextSplitter({
            chunkSize: 1000,
            chunkOverlap: 200,
            separators: ["\n\n", "\n", ".", "!", "?", ",", " ", ""],
        });

        const chunks = await splitter.createDocuments([extractedText]);
        console.log(`[AI-LAKE] Texto fragmentado em ${chunks.length} pedaÃ§os (Recursive Splitter).`);

        // 5. VetorizaÃ§Ã£o e Salvamento - BYPASS SDK (REST API)
        console.log(`[AI-LAKE] Gerando embeddings via REST API (us-central1)...`);

        // Setup Auth para REST
        const auth = new GoogleAuth({
            keyFile: serviceAccountPath,
            scopes: 'https://www.googleapis.com/auth/cloud-platform',
        });
        const authToken = await auth.getAccessToken();

        let savedChunks = 0;
        for (let i = 0; i < chunks.length; i++) {
            const chunkText = chunks[i].pageContent;

            // Chamada REST Direta para Vertex AI Embeddings
            const url = `https://us-central1-aiplatform.googleapis.com/v1/projects/${project}/locations/us-central1/publishers/google/models/text-embedding-004:predict`;

            const restResponse = await axios.post(url, {
                instances: [{ content: chunkText }]
            }, {
                headers: {
                    'Authorization': `Bearer ${authToken}`,
                    'Content-Type': 'application/json'
                }
            });

            const embedding = restResponse.data.predictions[0].embeddings.values;

            // Salvar no Firestore
            await db.collection('contratacoes')
                .doc(processId)
                .collection('arquivos')
                .doc(fileId)
                .collection('chunks')
                .add({
                    text: chunkText,
                    embedding: admin.firestore.FieldValue.vector(embedding),
                    chunkIndex: i,
                    processoId: processId,
                    fileId: fileId,
                    createdAt: admin.firestore.FieldValue.serverTimestamp()
                });

            savedChunks++;
            if (i % 5 === 0) console.log(`[AI-LAKE] Progresso: ${i + 1}/${chunks.length} chunks vetorizados...`);
        }

        // 6. FinalizaÃ§Ã£o com Sucesso
        await db.collection('contratacoes').doc(processId).collection('arquivos').doc(fileId).update({
            status: 'COMPLETED',
            totalChunks: savedChunks,
            processedAt: admin.firestore.FieldValue.serverTimestamp()
        });

        console.log(`[AI-LAKE] âœ… Processamento CONCLUÃDO para ${storagePath}`);

    } catch (error) {
        console.error(`[AI-LAKE] âŒ ERRO no processamento do arquivo:`, error.message);

        // Tenta registrar o erro no Firestore
        try {
            if (db && processId && fileId) {
                await db.collection('contratacoes').doc(processId).collection('arquivos').doc(fileId).update({
                    status: 'ERROR',
                    errorMessage: error.message,
                    processedAt: admin.firestore.FieldValue.serverTimestamp()
                });
            }
        } catch (e) {
            console.error('[AI-LAKE] Fatal: nÃ£o foi possÃ­vel salvar log de erro no DB.', e);
        }
    }
}

async function extractTextWithGemini(buffer, fileName) {
    const model = vertexAI.getGenerativeModel({
        model: 'gemini-3-flash-preview'
    });

    const isHtml = fileName.toLowerCase().endsWith('.html') || fileName.toLowerCase().endsWith('.htm');
    const base64Data = buffer.toString('base64');

    const request = {
        contents: [
            {
                role: 'user',
                parts: isHtml ? [
                    { text: `Abaixo estÃ¡ o conteÃºdo de um arquivo HTML extraÃ­do do SIPAC. Extraia e transcreva todo o texto relevante:\n\n${buffer.toString('utf-8')}` }
                ] : [
                    { inlineData: { data: base64Data, mimeType: 'application/pdf' } },
                    { text: "Transcreva todo o texto deste documento PDF de forma fiel. Se houver tabelas, tente manter a estrutura lÃ³gica." }
                ]
            }
        ],
        thinkingConfig: {
            includeThoughts: true,
            thinkingLevel: 'HIGH'
        },
        generationConfig: {
            temperature: 0,
            mediaResolution: 'MEDIA_RESOLUTION_HIGH'
        }
    };

    const response = await model.generateContent(request);
    return response.response.candidates[0].content.parts[0].text;
}

/**
 * FunÃ§Ã£o legado/opcional para resumo de despachos.
 * Mantida para evitar quebra de contratos de importaÃ§Ã£o no index.js
 */
export async function summarizeDespachos(processoInfo, documentos) {
    console.log(`[AI] Gerando resumo (Gemini 3 REST) para ${documentos.length} despachos...`);

    const context = `
    Processo: ${processoInfo.protocolo}
    Objeto: ${processoInfo.assunto || 'NÃ£o informado'}
    Unidade Atual: ${processoInfo.unidadeAtual || 'NÃ£o informado'}
    `;

    const despachosMarkdown = documentos.map(d => `
    --- DESPACHO (${d.data} - ${d.unidadeOrigem}) ---
    ${d.texto}
    `).join('\n');

    const prompt = `
    VocÃª Ã© um assistente especializado em gestÃ£o pÃºblica.
    Com base nas informaÃ§Ãµes do processo abaixo e nos textos dos despachos, gere:
    1. Um "Resumo Flash" de uma frase curta e impactante sobre o status atual.
    2. Um "RelatÃ³rio Detalhado" em markdown, descrevendo o histÃ³rico e os prÃ³ximos passos sugeridos.

    INFO PROCESSO:
    ${context}

    DESPACHOS:
    ${despachosMarkdown}
    `;

    try {
        // Setup Auth para REST
        const auth = new GoogleAuth({
            keyFile: serviceAccountPath,
            scopes: 'https://www.googleapis.com/auth/cloud-platform',
        });
        const authToken = await auth.getAccessToken();

        // URL Global para Gemini 3 (v1beta1 para experimental features)
        const url = `https://aiplatform.googleapis.com/v1beta1/projects/${project}/locations/global/publishers/google/models/gemini-3-flash-preview:streamGenerateContent`;

        const restResponse = await axios.post(url, {
            contents: {
                role: 'user',
                parts: [{ text: prompt }]
            },
            // Sintaxe exata do Manual Gemini 3
            generation_config: {
                temperature: 0.1,
                max_output_tokens: 8192
            }
        }, {
            headers: {
                'Authorization': `Bearer ${authToken}`,
                'Content-Type': 'application/json'
            }
        });

        // Como usamos streamGenerateContent, tratamos o array de retorno
        // (Simplificado para pegar o primeiro bloco de texto vÃ¡lido)
        let fullText = "";
        if (Array.isArray(restResponse.data)) {
            fullText = restResponse.data
                .map(chunk => chunk.candidates?.[0]?.content?.parts?.[0]?.text || "")
                .join("");
        } else {
            fullText = restResponse.data.candidates?.[0]?.content?.parts?.[0]?.text || "";
        }

        const flashmatch = fullText.match(/Resumo Flash[:\*]*\s*([^\n\.]+)/i);
        const flash = flashmatch ? flashmatch[1].trim() : "AnÃ¡lise concluÃ­da com sucesso.";

        return {
            resumoFlash: flash,
            relatorioDetalhado: fullText
        };
    } catch (error) {
        console.error('[AI SUMMARY REST ERROR]', error.response?.data || error.message);
        throw error;
    }
}

/**
 * INTERFACE DE CHAT (RAG - Retrieval Augmented Generation)
 * Busca trechos relevantes no Data Lake e responde usando Gemini 3.
 */
export async function chatWithAI(processId, userQuery, history = []) {
    console.log(`[AI-CHAT] ðŸ’¬ Pergunta recebida para o processo ${processId}: ${userQuery}`);
    const db = admin.firestore();

    try {
        // 1. GERA EMBEDDING DA PERGUNTA (via REST)
        const auth = new GoogleAuth({
            keyFile: serviceAccountPath,
            scopes: 'https://www.googleapis.com/auth/cloud-platform',
        });
        const authToken = await auth.getAccessToken();

        const embedUrl = `https://us-central1-aiplatform.googleapis.com/v1/projects/${project}/locations/us-central1/publishers/google/models/text-embedding-004:predict`;
        const embedResponse = await axios.post(embedUrl, {
            instances: [{ content: userQuery }]
        }, {
            headers: { 'Authorization': `Bearer ${authToken}`, 'Content-Type': 'application/json' }
        });

        const queryVector = embedResponse.data.predictions[0].embeddings.values;

        // 2. BUSCA SEMÃ‚NTICA NO FIRESTORE (Vector Search)
        // Buscamos em todos os chunks que pertencem a este processo
        console.log(`[AI-CHAT] ðŸ” Buscando fragmentos relevantes no Firestore...`);
        const chunksSnapshot = await db.collectionGroup('chunks')
            .where('processoId', '==', processId)
            .findNearest({
                vectorField: 'embedding',
                queryVector: queryVector,
                distanceMeasure: 'COSINE',
                limit: 8
            })
            .get();

        const contextChunks = chunksSnapshot.docs.map(doc => {
            const data = doc.data();
            // Tenta formatar o ID do arquivo para ficar legÃ­vel (Ex: "1-OFICIO" -> "Documento #1 (OFICIO)")
            let docLabel = data.fileId;
            if (/^\d+-/.test(data.fileId)) {
                const parts = data.fileId.split('-');
                const order = parts[0];
                const type = parts.slice(1).join(' ').replace(/_/g, ' ');
                docLabel = `Documento #${order} (${type})`;
            }

            return `[${docLabel}] ConteÃºdo: ${data.text}`;
        });

        if (contextChunks.length === 0) {
            console.warn(`[AI-CHAT] âš ï¸ Nenhum contexto encontrado para este processo.`);
        }

        // 3. MONTAGEM DO PROMPT PARA O GEMINI 3
        const contextText = contextChunks.join('\n\n');
        const systemPrompt = `
# PERSONA
VocÃª Ã© o "Consultor de Processos", um assistente de IA especializado em Direito Administrativo e LicitaÃ§Ãµes PÃºblicas. Sua funÃ§Ã£o Ã© auxiliar gestores a extrair informaÃ§Ãµes precisas de processos complexos.

# INSTRUÃ‡Ã•ES DE RESPOSTA
1. **FundamentaÃ§Ã£o ObrigatÃ³ria:** Responda EXCLUSIVAMENTE com base no CONTEXTO fornecido abaixo. Ignore seu conhecimento externo, a menos que o documento cite explicitamente uma lei.
2. **Estrutura da Resposta (Prioridade Executiva):**
   - Inicie com uma **Resposta Direta** e objetiva em negrito.
   - Em seguida, apresente o **Detalhamento** ou a lista de requisitos usando bullet points.
   - Se houver divergÃªncia entre documentos (ex: um Edital original e uma RetificaÃ§Ã£o posterior), aponte a discrepÃ¢ncia explicitamente.
3. **CitaÃ§Ã£o de Fontes:** Toda afirmaÃ§Ã£o factual (datas, valores, prazos, multas) deve vir acompanhada da referÃªncia entre colchetes com o formato "Documento {NÃšMERO} ({TIPO})". Exemplo: "O prazo Ã© de 5 dias [Documento 1 (EDITAL)]".
4. **Tratamento de AusÃªncias:** Se a informaÃ§Ã£o nÃ£o estiver no contexto, NÃƒO TENTE INFERIR. Responda: "A informaÃ§Ã£o solicitada nÃ£o consta nos trechos recuperados para esta anÃ¡lise."

# CONTEXTO DOS DOCUMENTOS (RAG):
${contextText}
`;

        // 4. CHAMADA AO GEMINI 3 FLASH PREVIEW (REST API)
        // Usamos v1beta1 para garantir suporte a funcionalidades experimentais como thinking_config
        const chatUrl = `https://aiplatform.googleapis.com/v1beta1/projects/${project}/locations/global/publishers/google/models/gemini-3-flash-preview:streamGenerateContent`;

        // Converte o histÃ³rico para o formato do Gemini
        const contents = history.map(msg => ({
            role: msg.role === 'user' ? 'user' : 'model',
            parts: [{ text: msg.content }]
        }));

        // Adiciona a pergunta atual com o prompt de sistema injetado no contexto
        contents.push({
            role: 'user',
            parts: [{ text: `CONTEXTO DO PROCESSO:\n${contextText}\n\nPERGUNTA: ${userQuery}` }]
        });

        const restResponse = await axios.post(chatUrl, {
            contents: contents,
            system_instruction: {
                parts: [{ text: systemPrompt }]
            },
            // Re-habilitando thinking_config com v1beta1
            generation_config: {
                temperature: 0,
                max_output_tokens: 4000
            }
        }, {
            headers: {
                'Authorization': `Bearer ${authToken}`,
                'Content-Type': 'application/json'
            }
        });

        let fullResponse = "";
        if (Array.isArray(restResponse.data)) {
            fullResponse = restResponse.data
                .map(chunk => chunk.candidates?.[0]?.content?.parts?.[0]?.text || "")
                .join("");
        } else {
            fullResponse = restResponse.data.candidates?.[0]?.content?.parts?.[0]?.text || "";
        }

        // 5. SALVAR NO HISTÃ“RICO (Opcional - pode ser feito no index.js ou aqui)
        await db.collection('contratacoes').doc(processId).collection('chat_history').add({
            role: 'assistant',
            content: fullResponse,
            timestamp: admin.firestore.FieldValue.serverTimestamp(),
            contextCount: contextChunks.length
        });

        const sources = chunksSnapshot.docs.map(doc => {
            const data = doc.data();
            return {
                id: doc.id,
                fileId: data.fileId,
                preview: data.text.slice(0, 150) + '...'
            };
        });

        // Deduplicate sources by fileId
        const uniqueSources = [...new Map(sources.map(item => [item.fileId, item])).values()];

        return {
            answer: fullResponse,
            citations: contextChunks.length,
            sources: uniqueSources
        };

    } catch (error) {
        console.error(`[AI-CHAT ERROR]`, error.response?.data || error.message);
        throw error;
    }
}
